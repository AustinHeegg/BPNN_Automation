# import numpy as np


# class BPNeuralNetwork:
#     def __init__(self, weights, biases):
#         self.weights = weights  # 权重矩阵
#         self.biases = biases  # 偏置向量

#     def activation_function(self, x):
#         """使用 sigmoid 激活函数进行激活."""
#         return 1 / (1 + np.exp(-x))

#     def forward_propagation(self, inputs):
#         """执行前向传播并返回输出层的结果."""
#         # 第一层（输入层到第一个隐藏层）
#         hidden_layer_1 = self.activation_function(np.dot(self.weights[0], inputs.T) + self.biases[0].reshape(-1, 1))

#         # 第二层（第一个隐藏层到第二个隐藏层）
#         hidden_layer_2 = self.activation_function(
#             np.dot(self.weights[1], hidden_layer_1) + self.biases[1].reshape(-1, 1))

#         # 第三层（第二个隐藏层到第三个隐藏层）
#         hidden_layer_3 = self.activation_function(
#             np.dot(self.weights[2], hidden_layer_2) + self.biases[2].reshape(-1, 1))

#         # 输出层（第三个隐藏层到输出层）
#         output_layer = np.dot(self.weights[3], hidden_layer_3) + self.biases[3].reshape(-1, 1)

#         return output_layer


# if __name__ == "__main__":
#     weights = [
#         np.array([
#                 [
#                     -0.5864076018333435,
#                     0.052436769008636475
#                 ],
#                 [
#                     -0.3801797032356262,
#                     0.47272300720214844
#                 ],
#                 [
#                     0.5131611227989197,
#                     -0.07084912806749344
#                 ],
#                 [
#                     0.4995582699775696,
#                     0.2373298704624176
#                 ],
#                 [
#                     -0.1812146157026291,
#                     -0.47772929072380066
#                 ],
#                 [
#                     -0.14977850019931793,
#                     0.5165948867797852
#                 ],
#                 [
#                     -0.4049115478992462,
#                     -0.41056257486343384
#                 ],
#                 [
#                     -0.43457308411598206,
#                     0.3835880160331726
#                 ]
#             ]),
#         np.array([
#                 [
#                     -0.4698057472705841,
#                     -0.5924273133277893,
#                     0.49930429458618164,
#                     0.8603581190109253,
#                     0.43089035153388977,
#                     -0.46999457478523254,
#                     -0.7337070107460022,
#                     -0.9591380953788757
#                 ],
#                 [
#                     -0.2004239857196808,
#                     0.4326987564563751,
#                     0.3369041681289673,
#                     0.6039580702781677,
#                     -1.284999132156372,
#                     0.5409873723983765,
#                     -0.9662082195281982,
#                     0.14294761419296265
#                 ],
#                 [
#                     0.5066338777542114,
#                     0.08376629650592804,
#                     -0.4340434670448303,
#                     -0.9113590717315674,
#                     0.7391244769096375,
#                     -0.5016915202140808,
#                     1.130360722541809,
#                     0.2911302149295807
#                 ],
#                 [
#                     0.328965425491333,
#                     0.46899333596229553,
#                     -0.4218129813671112,
#                     -0.13314507901668549,
#                     -0.12185458093881607,
#                     0.4735705256462097,
#                     0.1418370008468628,
#                     0.6502296328544617
#                 ],
#                 [
#                     -0.1515417993068695,
#                     -0.28520333766937256,
#                     -0.011243864893913269,
#                     0.13228192925453186,
#                     0.4738929867744446,
#                     -0.6000354886054993,
#                     0.3337218463420868,
#                     -0.7063257694244385
#                 ],
#                 [
#                     -0.5606960654258728,
#                     -0.4832722544670105,
#                     0.585197389125824,
#                     0.5723952651023865,
#                     -0.3527105152606964,
#                     0.19380532205104828,
#                     -0.7833530306816101,
#                     -0.6510854363441467
#                 ],
#                 [
#                     0.10969336330890656,
#                     -0.6170898079872131,
#                     -0.09027018398046494,
#                     -0.2637086510658264,
#                     1.1090104579925537,
#                     -0.7182928919792175,
#                     0.8936255574226379,
#                     -0.42702212929725647
#                 ],
#                 [
#                     0.13297386467456818,
#                     0.48375675082206726,
#                     -0.6688300967216492,
#                     -0.25804680585861206,
#                     -0.6859254837036133,
#                     1.0781662464141846,
#                     -0.38774365186691284,
#                     1.0430554151535034
#                 ]
#             ]),
#         np.array([
#                 [
#                     1.0326215028762817,
#                     -1.0268703699111938,
#                     0.15443722903728485,
#                     -0.5980297923088074,
#                     0.8370410799980164,
#                     0.4589509665966034,
#                     0.7349340319633484,
#                     -1.6390628814697266
#                 ],
#                 [
#                     0.7565467357635498,
#                     1.0361459255218506,
#                     -0.8623573780059814,
#                     -0.5429003834724426,
#                     -0.03424008563160896,
#                     0.6624435782432556,
#                     -0.6398899555206299,
#                     -0.057882849127054214
#                 ],
#                 [
#                     -0.3958200514316559,
#                     -1.070389986038208,
#                     0.9977619051933289,
#                     0.0008036756189540029,
#                     0.16878581047058105,
#                     -0.6913771033287048,
#                     0.762683093547821,
#                     -0.45711272954940796
#                 ],
#                 [
#                     -1.2758711576461792,
#                     0.1435859054327011,
#                     0.5368182063102722,
#                     0.8396580219268799,
#                     -0.39511483907699585,
#                     -0.6623274087905884,
#                     -0.8043721318244934,
#                     0.9027481079101562
#                 ],
#                 [
#                     -1.223425030708313,
#                     -0.7072770595550537,
#                     1.1514467000961304,
#                     0.26982855796813965,
#                     -0.12289702892303467,
#                     -1.1928437948226929,
#                     0.2630283534526825,
#                     0.9068532586097717
#                 ],
#                 [
#                     0.5643081068992615,
#                     0.4189562499523163,
#                     -0.7692531943321228,
#                     0.043961331248283386,
#                     -0.1925084888935089,
#                     0.4156278967857361,
#                     -0.4033801853656769,
#                     -0.01420468557626009
#                 ],
#                 [
#                     -0.059381626546382904,
#                     -1.23622727394104,
#                     1.1121243238449097,
#                     -0.302335262298584,
#                     0.5376720428466797,
#                     -0.5511724948883057,
#                     1.465455174446106,
#                     -1.106412410736084
#                 ],
#                 [
#                     0.8132088780403137,
#                     -0.21575546264648438,
#                     0.1912275105714798,
#                     -0.5873798727989197,
#                     0.2556914985179901,
#                     0.29847607016563416,
#                     0.47198352217674255,
#                     -0.7053456902503967
#                 ]
#             ]),
#         np.array([
#                 [
#                     1.4318506717681885,
#                     1.7961044311523438,
#                     -1.1991246938705444,
#                     -1.8298602104187012,
#                     -2.5889313220977783,
#                     1.0680423974990845,
#                     -0.6951248049736023,
#                     0.9507432579994202
#                 ],
#                 [
#                     -2.279205322265625,
#                     1.2924069166183472,
#                     -1.3044933080673218,
#                     1.5632522106170654,
#                     0.06312528997659683,
#                     0.9128340482711792,
#                     -2.6682868003845215,
#                     -0.8634008765220642
#                 ]
#             ])
#     ]

#     biases = [
#         np.array([
#                 0.26315930485725403,
#                 0.012200870551168919,
#                 0.1034495085477829,
#                 -0.0030445526354014874,
#                 0.2297627329826355,
#                 -0.12308595329523087,
#                 -0.25460097193717957,
#                 -0.5442857146263123
#             ]),
#         np.array([
#                 0.5885407328605652,
#                 -0.08803900331258774,
#                 -0.23263326287269592,
#                 -0.2396632879972458,
#                 0.18997596204280853,
#                 0.38990694284439087,
#                 -0.00948682613670826,
#                 -0.06388009339570999
#             ]),
#         np.array([
#                 0.04386699199676514,
#                 0.21648052334785461,
#                 0.13645915687084198,
#                 0.20411226153373718,
#                 -0.0373486690223217,
#                 0.3357012867927551,
#                 -0.03419256582856178,
#                 -0.2513905167579651
#             ]),
#         np.array([
#                 0.08156203478574753,
#                 1.3403139114379883
#             ])
#     ]

#     # 输入数据
#     input_data = np.array([
#         [2.79893862081549, -0.39932882084621],
#         [2.39993491496712, -0.39932882084621],
#         [1.99970672260403, -0.39932882084621],
#         [1.59940262546915, -0.39932882084621],
#         [1.19903781828619, -0.39932882084621],
#         [0.799761854932068,- 0.39932882084621],
#         [0.39932128487282, -0.39932882084621],
#         [0, -0.39932882084621],
#         [-0.39932128487282, -0.39932882084621],
#         [-0.799761854932068, -0.39932882084621],
#         [-1.19903781828619, -0.39932882084621],
#         [-1.59940262546915, -0.39932882084621],
#         [-1.99970672260403, -0.39932882084621],
#         [-2.39993491496712, -0.39932882084621],
#         [-2.79893862081549, -0.39932882084621]
#     ])

#     # 创建 BP 神经网络实例
#     bp_nn = BPNeuralNetwork(weights, biases)

#     # 对每个输入进行前向传播并输出结果
#     outputs = bp_nn.forward_propagation(input_data)

#     print("输出结果:")
#     print(outputs.T)


import numpy as np
import pdb

from sympy.codegen.cnodes import sizeof


class BPNN:
    def __init__(self):
        # 设置随机种子以便重现
        np.random.seed(42)

        self.input_layer_size = 2
        self.hidden_layer_size = 8
        self.output_layer_size = 2

        # 初始化权重（使用所提供的权重）
        self.weights = [
            np.array([
                [-0.5864076018333435, 0.052436769008636475],
                [-0.3801797032356262, 0.47272300720214844],
                [0.5131611227989197, -0.07084912806749344],
                [0.4995582699775696, 0.2373298704624176],
                [-0.1812146157026291, -0.47772929072380066],
                [-0.14977850019931793, 0.5165948867797852],
                [-0.4049115478992462, -0.41056257486343384],
                [-0.43457308411598206, 0.3835880160331726]
            ]),
            np.array([ # 第二层的权重
                [-0.4698057472705841, -0.5924273133277893, 0.49930429458618164, 0.8603581190109253, 0.43089035153388977, -0.46999457478523254, -0.7337070107460022, -0.9591380953788757],
                [-0.2004239857196808, 0.4326987564563751, 0.3369041681289673, 0.6039580702781677, -1.284999132156372, 0.5409873723983765, -0.9662082195281982, 0.14294761419296265],
                [0.5066338777542114, 0.08376629650592804, -0.4340434670448303, -0.9113590717315674, 0.7391244769096375, -0.5016915202140808, 1.130360722541809, 0.2911302149295807],
                [0.328965425491333, 0.46899333596229553, -0.4218129813671112, -0.13314507901668549, -0.12185458093881607, 0.4735705256462097, 0.1418370008468628, 0.6502296328544617],
                [-0.1515417993068695, -0.28520333766937256, -0.011243864893913269, 0.13228192925453186, 0.4738929867744446, -0.6000354886054993, 0.3337218463420868, -0.7063257694244385],
                [-0.5606960654258728, -0.4832722544670105, 0.585197389125824, 0.5723952651023865, -0.3527105152606964, 0.19380532205104828, -0.7833530306816101, -0.6510854363441467],
                [0.10969336330890656, -0.6170898079872131, -0.09027018398046494, -0.2637086510658264, 1.1090104579925537, -0.7182928919792175, 0.8936255574226379, -0.42702212929725647],
                [0.13297386467456818, 0.48375675082206726, -0.6688300967216492, -0.25804680585861206, -0.6859254837036133, 1.0781662464141846, -0.38774365186691284, 1.0430554151535034]
            ]),
            np.array([
                [1.0326215028762817, -1.0268703699111938, 0.15443722903728485, -0.5980297923088074, 0.8370410799980164, 0.4589509665966034, 0.7349340319633484, -1.6390628814697266],
                [0.7565467357635498, 1.0361459255218506, -0.8623573780059814, -0.5429003834724426, -0.03424008563160896, 0.6624435782432556, -0.6398899555206299, -0.057882849127054214],
                [-0.3958200514316559, -1.070389986038208, 0.9977619051933289, 0.0008036756189540029, 0.16878581047058105, -0.6913771033287048, 0.762683093547821, -0.45711272954940796],
                [-1.2758711576461792, 0.1435859054327011, 0.5368182063102722, 0.8396580219268799, -0.39511483907699585, -0.6623274087905884, -0.8043721318244934, 0.9027481079101562],
                [-1.223425030708313, -0.7072770595550537, 1.1514467000961304, 0.26982855796813965, -0.12289702892303467, -1.1928437948226929, 0.2630283534526825, 0.9068532586097717],
                [0.5643081068992615, 0.4189562499523163, -0.7692531943321228, 0.043961331248283386, -0.1925084888935089, 0.4156278967857361, -0.4033801853656769, -0.01420468557626009],
                [-0.059381626546382904, -1.23622727394104, 1.1121243238449097, -0.302335262298584, 0.5376720428466797, -0.5511724948883057, 1.465455174446106, -1.106412410736084],
                [0.8132088780403137, -0.21575546264648438, 0.1912275105714798, -0.5873798727989197, 0.2556914985179901, 0.29847607016563416, 0.47198352217674255, -0.7053456902503967]
            ]),
            np.array([
                [1.4318506717681885, 1.7961044311523438, -1.1991246938705444, -1.8298602104187012, -2.5889313220977783, 1.0680423974990845, -0.6951248049736023, 0.9507432579994202],
                [-2.279205322265625, 1.2924069166183472, -1.3044933080673218, 1.5632522106170654, 0.06312528997659683, 0.9128340482711792, -2.6682868003845215, -0.8634008765220642]
            ])
        ]

        # 偏置的初始化
        self.biases = [
            np.array([0.26315930485725403, 0.012200870551168919, 0.1034495085477829, -0.0030445526354014874, 0.2297627329826355, -0.12308595329523087, -0.25460097193717957, -0.5442857146263123]),
            np.array([0.5885407328605652, -0.08803900331258774, -0.23263326287269592, -0.2396632879972458, 0.18997596204280853, 0.38990694284439087, -0.00948682613670826, -0.06388009339570999]),
            np.array([0.04386699199676514, 0.21648052334785461, 0.13645915687084198, 0.20411226153373718, -0.0373486690223217, 0.3357012867927551, -0.03419256582856178, -0.2513905167579651]),
            np.array([0.08156203478574753, 1.3403139114379883])
        ]

    def sigmoid(self, z):
        """Sigmoid激活函数"""
        return 1 / (1 + np.exp(-z))

    def forward(self, X):
        """正向传播计算"""
        # 输入层到第一个隐藏层
        # pdb.set_trace()
        self.hidden_layer1 = self.sigmoid(np.dot(X, self.weights[0].T) + self.biases[0])  # 使用转置
        print("Calculating Hidden Layer 1:")
        print("z1 = X * W1^T + b1")
        print("Where:")
        print(f"X = {X}")
        print(f"W1 = {self.weights[0].T}")
        print(f"b1 = {self.biases[0]}")
        print("Output of Hidden Layer 1:")
        print(self.hidden_layer1)
        print(self.hidden_layer1.shape)

        # 第一个隐藏层到第二个隐藏层
        self.hidden_layer2 = self.sigmoid(np.dot(self.hidden_layer1, self.weights[1].T) + self.biases[1])  # 使用转置
        # print("\nCalculating Hidden Layer 2:")
        # print("z2 = h1 * W2^T + b2")
        # print("Where:")
        print(f"h1 = {self.hidden_layer1}")
        print(f"W2 = {self.weights[1].T}")
        print(self.hidden_layer2)

        # print(f"b2 = {self.biases[1]}")
        # print("Output of Hidden Layer 2:")
        # print(self.hidden_layer2)
        # print(self.hidden_layer2.shape)


        # 第2个隐藏层到第3个隐藏层
        # self.hidden_layer3 = self.sigmoid(np.dot(self.hidden_layer2, self.weights[2].T) + self.biases[2])  # 使用转置
        # print("\nCalculating Hidden Layer 2:")
        # print("z2 = h1 * W2^T + b2")
        # print("Where:")
        # print(f"h1 = {self.hidden_layer1}")
        # print(f"W2 = {self.weights[2].T}")
        # print(f"b2 = {self.biases[2]}")
        # print("Output for Hidden Layer 2:")
        # print(self.hidden_layer3)
        # print(self.hidden_layer3.shape)


        # 第3个隐藏层到输出层
        # self.output_layer = np.dot(self.hidden_layer3, self.weights[3].T) + self.biases[3]  # 使用转置
        # print("\nCalculating Output Layer:")
        # print("z3 = h2 * W3^T + b3")
        # print("Where:")
        # print(f"h2 = {self.hidden_layer2}")
        # print(f"W3 = {self.weights[2].T}")
        # print(f"b3 = {self.biases[2]}")
        # print("Biases for Output Layer:")
        # print(self.biases[2])

        # return self.output_layer


# 实例化神经网络
bpnn = BPNN()

# 示例输入数据
input_data = np.array([[2.79893862081549, -0.39932882084621]])
                       # [2.39993491496712, -0.39932882084621],
                       # [1.99970672260403, -0.39932882084621],
                       # [1.59940262546915, -0.39932882084621],
                       # [1.19903781828619, -0.39932882084621],
                       # [0.799761854932068, -0.39932882084621],
                       # [0.39932128487282, -0.39932882084621],
                       # [0, -0.39932882084621],
                       # [-0.39932128487282, -0.39932882084621],
                       # [-0.799761854932068, -0.39932882084621],
                       # [-1.19903781828619, -0.39932882084621],
                       # [-1.59940262546915, -0.39932882084621],
                       # [-1.99970672260403, -0.39932882084621],
                       # [-2.39993491496712, -0.39932882084621],
                       # [-2.79893862081549, -0.39932882084621],
                       # [0, 0]])  # 16个输入样本

# 执行正向传播
output_data = bpnn.forward(input_data)

print("Final Output of the BP Neural Network:")
print(output_data)